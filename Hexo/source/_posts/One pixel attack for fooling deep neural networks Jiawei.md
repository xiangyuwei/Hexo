title: One pixel attack for fooling deep neural networks Jiawei

tags: 
- 对抗样本

categories:
- 论文

---

## 摘要
在图像识别领域，基于 DNN 的方法突破了传统的图像处理技术，达到了可与人类媲美的结果 [9]。但是，多个研究证明自然图像中的人工扰动可以轻易使 DNN 对图像进行错误分类，研究者提出了生成此类「对抗图像」的高效算法<!--more--> [1, 2, 3, 4]。生成对抗图像（adversarial images）的主要方式是向准确分类的自然图像中添加精心设计的额外扰动，该扰动不影响人类对图像的识别。这样的修改导致分类器将修改后的图像标注为完全不同的其他物体。但是，大多数之前的攻击并未考虑非常有限的对抗实例，即扰动的量有时候也会影响到人眼的识别能力（示例见图 2）。此外，研究在有限场景中创建的对抗图像更加有趣，因为它们可能更接近原始类别和目标类别（target class）之间的边界，研究此类关键点可以使人类更多地了解 DNN 输入空间的几何特征
## 方法
### 贡献
>非目标攻击，修改1,3,5个像素
黑盒攻击
可攻击不同模型
对特征目标有相同的扰动方向


### 方法
它不抽象的问题
将扰动搜索到任意显式的目标函数进行求解，而直接关注提高目标类别的概率标注值
全局对抗扰动可以泄露标签
 固定修改像素的个数，增改像素改变的强度
 Differential evolution (DE) which belongs to the general
class of genetic algorithm (GA
## 实验
## 结论




