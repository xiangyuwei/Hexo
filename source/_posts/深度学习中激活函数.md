# 深度学习中激活函数

标签（空格分隔）： 未分类

---


在神经网络中，激活函数的作用是能够给神经网络加入一些非线性因素，使得神经网络可以更好地解决较为复杂的问题。
激活函数众所周知有tanh,sigmoid,ReLU等。

tanh　　　双切正切函数，取值范围[-1,1]

sigmoid　 采用S形函数，取值范围[0,1]

ReLU  取的max(0,x)    简单而粗暴，大于0的留下，否则一律为0。

可微函数是指那些在定义域中所有点都存在导数的函数。

[知乎](https://www.zhihu.com/question/22334626)忆臻的回答中激活函数是多个线性直线的组合，图像比较直观
[经网络中的激活函数（activation function）-Sigmoid, ReLu, TanHyperbolic(tanh)]( softmax, softplushttp://blog.csdn.net/qrlhl/article/details/60883604)

[梯度消失问题](http://www.cnblogs.com/rgvb178/p/6055213.html),由于激活函数的饱和性。
[几种常见的激活函数](http://blog.csdn.net/u014365862/article/details/52710698)从神经网络的构建讲解激活函数。