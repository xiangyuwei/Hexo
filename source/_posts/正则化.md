
#title

title: 正则化
# 所属分类

categories:

- 深度学习

# 标签(多个标签如下所示)

tags:

- 深度学习

- 对抗样本


------
---

正则化是为了解决过拟合问题。在Andrew Ng的机器学习视频中有提到（详见http://www.cnblogs.com/jianxinzhou/p/4083921.html）。
 正则化产生了[稀疏性（Sparsity）](http://blog.csdn.net/gshgsh1228/article/details/52199870)，减少了特征向量个数，降低了模型的复杂度。
 例如一回归问题，假设回归模型为：y=w1*x1+w2*x2+…+w1000*x1000+b

通过学习，如果最后学习到的w*就只有很少的非零元素，大部分W*为0或接近于0，例如只有5个非零的wi，那可以认为y之于这5个（因素）xi有关系，这更有利于人们对问题的认识和分析，抓住影响问题的主要方面（因素）更符合人们的认知习惯。
(1)L0范数

    L0范数是指向量中非零元素的个数。L0正则化的值是模型中非零参数的个数，L0正则化可以实现模型参数的的稀疏化
（2）L1范数
 L1范数是指向量中各个元素绝对值之和，又叫“稀疏规则算子”（Lasso regularization）。
 
[L1正则化和L2正则化可以看做是损失函数的惩罚项](http://blog.csdn.net/jinping_shi/article/details/52433975)。所谓『惩罚』是指对损失函数中的某些参数做一些限制。
[知乎](https://www.zhihu.com/question/20924039)中陶轻松的答案指出正则项使W向量中项的个数最小化，从而防止过拟合