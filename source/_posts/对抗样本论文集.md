#title

title: 对抗样本论文
# 所属分类

categories:

- 深度学习

# 标签(多个标签如下所示)

tags:

- 深度学习

- 对抗样本


------
## 深度学习
###　１、2014-ｅxlaining and Harnessing Adversarial Examples（对抗样本的解释和利用）”[[ArXiv](http://arxiv.org/abs/1412.6572)][[code](https://github.com/rodgzilla/machine_learning_adversarial_examples)][[code](https://github.com/tensorflow/cleverhans/blob/master/cleverhans/attacks.py)]

提出对抗样本不是因为非线性，而是因为模型在高维空间中的线性性。
FSM 方法是计算关于x的导数，然后ｓｉｇｎ函数取值进行扰动,在$L_{无穷}$距离度量下。
$X^{adv}＝Ｘ＋ \varepsilon \cdot sign (\bigtriangledown \ J(X,y_{true}))$
梯度可以通过反向传播算法（backpropagation）计算获得。



###　２、2015-DeepFool: a simple and accurate method to fool deep neural networks [[ArXiv](http://arxiv.org/abs/1511.04599)][[code](https://github.com/tensorflow/cleverhans/blob/master/cleverhans/attacks.py)]
基于$L_2$距离度量的非目标攻击
$min ||D(x)-X||_2
subject\ to\ argmaxf(D(X) \neq\ y_{true})
$

###３、2015-The Limitations of Deep Learning in Adversarial Settings[[ArXiv](http://arxiv.org/abs/1511.07528)][[code](https://github.com/tensorflow/cleverhans)]

通过计算对抗显著图，寻找对输出影响最大的两个输入，每次迭代修改两个像素点，直到达到目标图像。
L0 distance metric

### ４、2015-Distributional Smoothing with Virtual Adversarial Training [[ArXiv](http://arxiv.org/abs/1507.00677)][[code](https://github.com/tensorflow/cleverhans/blob/master/cleverhans/attacks.py)]
KL散度

### ５、2016-Exploring the space of adversarial images[[ArXiv](http://arxiv.org/abs/1510.05328)][[code](https://github.com/xiangyuwei/adversarial-1)]-lua

###６、2016-Adversarial Machine Learning at Scale[[ArXiv](http://arxiv.org/abs/1611.01236)][[code](https://github.com/tensorflow/cleverhans/tree/master/examples/nips17_adversarial_competition/sample_defenses/adv_inception_v3)]

###７、2016-Delving into Transferable Adversarial Examples and Black-box Attacks [[ArXiv](http://arxiv.org/abs/1611.02770)][[code](https://github.com/sunblaze-ucb/transferability-advdnn-pub)]

### ８、2016-Practical Black-Box Attacks against Machine Learning[[ArXiv](http://arxiv.org/abs/1602.02697)][[code](https://github.com/tensorflow/cleverhans/blob/master/cleverhans_tutorials/mnist_blackbox.py)]
敌手不需要知道目标模型的结构和参数，只能够根据输入得到输出。
由于对抗样本的迁移性，可以先构建替代模型训练对抗样本再在目标模型上测试生成的对抗样本。
替代模型的结构和目标模型结构要能够处理相同的任务。


###９、2016-Adversarial examples in the physical world [[ArXiv](http://arxiv.org/abs/1607.02533)][[code](https://github.com/tensorflow/cleverhans/tree/master/examples/nips17_adversarial_competition/sample_targeted_attacks/iter_target_class)][[code](https://github.com/tensorflow/cleverhans/blob/master/cleverhans/attacks.py)]

通过对对抗样本拍照片，改变了某些像素来测试对抗样本的有效性
拓展FGSM,多次应用，每次一小步,中间结果被$\varepsilon$截断
$X_0^{adv}=X
 \\X_{N+1}^{adv}=Clip_{X,\varepsilon}\{X_{N}^{adv}+\alpha\cdot sign (\bigtriangledown J(X,y_{true}))\}
$

###10、2016-Simple Black-Box Adversarial Perturbations for Deep Networks[[ArXiv](http://arxiv.org/abs/1612.06299)]

###11、2017-Towards Evaluating the Robustness of Neural Networks[[ArXiv](http://arxiv.org/abs/608.04644)][[code](https://github.com/xiangyuwei/nn_robust_attacks)][[code](https://github.com/tensorflow/cleverhans/blob/master/cleverhans/attacks.py)]
It is noteworthy that the original defensive distillation is broken by Carlini and Wagner’s attack 
$min ||1/2(tanh(W)+1)-x)||^2_2+c\cdot l(1/2(tanh(W)+1))$

###12、2017-Fast Feature Fool: A data independent approach to universal adversarial perturbations[[ArXiv](http://arxiv.org/abs/1707.05572)][[code](https://github.com/xiangyuwei/fast-feature-fool)]

###13、2017-Towards Deep Learning Models Resistant to Adversarial Attacks[[ArXiv](http://arxiv.org/abs/1706.06083)][[code](https://github.com/tensorflow/cleverhans/blob/master/cleverhans/attacks.py)]

###14、2017-Ensemble Adversarial Training: Attacks and Defenses[[ArXiv](http://arxiv.org/abs/1705.07204)][[code](https://github.com/tensorflow/cleverhans/tree/master/examples/nips17_adversarial_competition/sample_defenses/ens_adv_inception_resnet_v2)]

###15、S. Zagoruyko. Are deep learning algorithms easily hackable?[[code](http://coxlab.github.io/ostrichinator)]

###16、2015-Deep neural networks are easily fooled: High confidence predictions for unrecognizable images[[ArXiv](http://arxiv.org/abs/1412.1897)]

###16、2016-Transferability in Machine Learning: from Phenomena to Black-Box Attacks using Adversarial Samples[[ArXiv](https://arxiv.org/abs/1605.07277)]

###17、2013-Intriguing properties of neural networkss[[ArXiv](http://arxiv.org/abs/1312.6199)]
通过在$Ｌ_２$距离度量下Ｌ—BFGS解决优化问题来生成对抗样本。
 　$min\ c|r|+loss_f(x+r,l)\  s.t\ x+r \subset[0,1]^m$
 　L-BFGS attack and DeepFool attacks have been implemented in the Foolbox[[code](https://github.com/xiangyuwei/foolbox)]
 　
 ### Evasion attacks against machine learning at test time
 adversary goal（攻击目标）:    分为两种目标，一是能够分类为好的即可（在垃圾邮件检测中，指分类为好的邮件）。如果有一个函数g定义在样本空间中X，g(x)<0表示分类为好，那么敌人的目标就是设计一个样本x*，让g(x*)<0。这种攻击很容易破解，只要调整分类界限即可（这里指g(x)=0这条边界）。所以，其实攻击者还有一个更好的目标是让g(x)尽可能小
##隐私问题
### 1、Hacking Smart Machines with Smarter Ones : How to Extract Meaningful Data from Machine Learning Classifiers[[ArXiv](http://arxiv.org/abs/1306.4447v1)]

###　２、Membership Inference Attacks Against Machine Learning Models[[ArXiv](http://arxiv.org/abs/1610.05820v2)]

### 3、Stealing machine learning models via prediction apis

### 4、An end-to-end case study of personal- ized warfarin dosing

## 其他机器学习技术
### 1、Vulnerability of Deep Reinforcement Learning to Policy Induction Attacks[[ArXiv](http://arxiv.org/abs/1701.04143)]
### 2、Tactics of Adversarial Attack on Deep Reinforcement Learning Agents[[ArXiv](http://arxiv.org/abs/1703.06748)]

### 3、Adversarial examples for generative models[[ArXiv](http://arxiv.org/abs/1702.06832)]
##防御技术
### 1、2016-Distillation as a Defense to Adversarial Perturbations Against Deep Neural Networks[[ArXiv](http://arxiv.org/abs/1511.04508)]
## 机器学习
### 1、2006-Can machine learning be secure?[[paper](http://portal.acm.org/citation.cfm?doid=1128817.1128824)]

### 2、2011-Adversarial machine learning

### 3、Take two software updates and see me in the morning: The Case for Software Security Evaluations of Medical Devices [[paper](http://www.contrib.andrew.cmu.edu/~ppoosank/papers/hanna-aed-healthsec11.pdf)]
### 4、Adversarial AI
###5、2017-机器学习安全性问题及其防御技术研究综述
### 6、Concrete Problems in AI Safety[[ArXiv](http://arxiv.org/abs/1606.06565)]
### 7、Towards the Science of Security and Privacy in Machine Learning[[ArXiv](http://arxiv.org/abs/1611.03814)]

###8、Stealing Machine Learning Models via Prediction APIs[[paper](https://www.usenix.org/conference/usenixsecurity16/technical-sessions/presentation/tramer)][[code](https://github.com/ftramer/Steal-ML)]

##GAN和对抗样本

### 1、2017-APE-GAN : Adversarial Perturbation Elimination with　GAN[[ArXiv](http://arxiv.org/abs/1707.05474v3)][[code](https://github.com/shenqixiaojiang/APE-GAN)]
数据集：including MNIST, CIFAR10 and ImageNet


### 2、2017-Generative Adversarial Trainer : Defense to Adversarial Perturbations with GAN[[ArXiv](http://arxiv.org/abs/1705.03387v1)]

### ３、2017-Generating Adversarial Malware Examples for Black-Box Attacks Based on GAN[[ArXiv](http://arxiv.org/abs/1702.05983)]
GAN也可以产生针对恶意 软件分类器的对抗样本
在提取的恶意软件特征上增加噪声使分类器误分类。

### ４、2017-Adversarial examples for generative models[[ArXiv](http://arxiv.org/abs/1702.06832)]

### 5、Generative Poisoning Attack Method Against Neural Networks[[ArXiv](http://arxiv.org/abs/1703.01340)]
提出了 一种更有效的产生对抗样本的方法，它采用生成对 抗网络（GAN）中生成模型（Generative Model）和 判别模型（Discriminative model）的思想，即先用生成模型产生候选的对抗样本，再用判别模型进行 筛选最优对抗样本。